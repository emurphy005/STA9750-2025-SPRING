[
  {
    "objectID": "Testpptx.html#part-1",
    "href": "Testpptx.html#part-1",
    "title": "Test Preso - Houston housing market",
    "section": "Part 1",
    "text": "Part 1\nThis slide shows the annual houses sold in Houston, TX from the year 2000-2015."
  },
  {
    "objectID": "Testpptx.html#part-2",
    "href": "Testpptx.html#part-2",
    "title": "Test Preso - Houston housing market",
    "section": "Part 2",
    "text": "Part 2\nThis slide shows the Average Price of houses sold by month."
  },
  {
    "objectID": "Testpptx.html#part-3",
    "href": "Testpptx.html#part-3",
    "title": "Test Preso - Houston housing market",
    "section": "Part 3",
    "text": "Part 3\nThis slide shows the average price of houses sold in 2015 by city."
  },
  {
    "objectID": "Testpptx.html#conclusion",
    "href": "Testpptx.html#conclusion",
    "title": "Test Preso - Houston housing market",
    "section": "Conclusion",
    "text": "Conclusion\nThis test presentation shows the variations of housing prices in the city of Houston, TX. It has covered the amount of houses sold over time, the average price by month, and the average price by city."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erin Murphy",
    "section": "",
    "text": "Hi there! My name is Erin and welcome to my page!\nLocated in Brooklyn, NY, I am currently a masters student at Baruch College working towards a MS degree in Business Analytics with a concentration in Marketing.\n\nLast Updated: Saturday 02 08, 2025 at 22:01PM"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini Project 1: NYC Taxpayer Spending Analysis",
    "section": "",
    "text": "New York City’s Commission to Analyze Taxpayer Spending (CATS) has tasked our team to help the Commmissioners indentify efficiencies and new opportunites for spending taypayer money through the analysis of City payroll data. This will be done through the analysis of the payscales, salary caps, and rentention rates to see how changes in any of these areas can help the City spend money more effiently."
  },
  {
    "objectID": "mp01.html#policy-i",
    "href": "mp01.html#policy-i",
    "title": "Mini Project 1: NYC Taxpayer Spending Analysis",
    "section": "Policy I:",
    "text": "Policy I:\nNew York City reports all pay for all indivduals, which allows for analysis of specific positions and . For example, included in this document is the employeement"
  },
  {
    "objectID": "mp01.html#policy-icapping-salaries-at-mayoral-level",
    "href": "mp01.html#policy-icapping-salaries-at-mayoral-level",
    "title": "Mini Project 1: NYC Taxpayer Spending Analysis",
    "section": "Policy I:Capping Salaries at Mayoral Level",
    "text": "Policy I:Capping Salaries at Mayoral Level\nNew York City reports all pay for all indivduals, which allows for analysis of specific positions and . For example, included in this document is the employeement"
  },
  {
    "objectID": "mp01.html#policy-i-capping-salaries-at-mayoral-level",
    "href": "mp01.html#policy-i-capping-salaries-at-mayoral-level",
    "title": "Mini Project 1: NYC Taxpayer Spending Analysis",
    "section": "Policy I: Capping Salaries at Mayoral Level",
    "text": "Policy I: Capping Salaries at Mayoral Level\nIn New York City, the Mayor position is a salaried position. The current mayor of New York City is Eric Adams. His employement and pay history with the City can be seen below:\n\n\n\n\n\n\n However, he is not the highest paid employee in the City. There are other employees that have a higher total compensation than the mayor.\nThe top 5 people who were paid more than the mayor in any given year are:\n\n\n\n\n\n\n This policy suggest the City should cap salaries at the mayoral level, which means no other person would have a salary more than the mayor’s.\nIf you were to cap all of the salaries above the mayoral level at the mayor level each year, during this ten year period, there would have been a total savings of $700,277.\n The Agencies and Positions most affected by this policy implementation would be:\n\n\n\n\n\n\n Since only 3 specific posistion would be affected, it would benefit the city to cap the pay. This would allow for the savings to be distributed to other areas and programs in the city that need it."
  },
  {
    "objectID": "mp01.html#policy-ii-increasing-staffing-to-reduce-overtime-expenses",
    "href": "mp01.html#policy-ii-increasing-staffing-to-reduce-overtime-expenses",
    "title": "Mini Project 1: NYC Taxpayer Spending Analysis",
    "section": "Policy II: Increasing Staffing to Reduce Overtime Expenses",
    "text": "Policy II: Increasing Staffing to Reduce Overtime Expenses\nWhen analyzing the data of the employee overtime pay of New York City, it quickly comes apparent that there are certain positions and agencies that accumulate more overtime hours than others. This second policy suggested the solution for this is to hire more full time employees to cover those extra hours, which will reduce the extra overtime costs.\nFor the sake of this analysis it it assummed that overtime pay is 1.5x the employee’s base pay and that the standard work week for all employees is 37.5 (7.5 hours a day). The average hourly rate was calculated for all employees, including those who are salaried or have a day rate.\nThrough inital exploration, at the agency level, the Police Department, Fire Department, and Department of Corrections have the positions that utilize the most overtime. The specific jobs positions accumulating the most overtime are:\n\n\n\nPolice Department\nFire Department\nDepartment of Corrections\n\n\n\n\nPolice Officer\nFireFighter\nCorrenction Officer\n\n\nSergeant\nLieutenant\nSanitation Worker\n\n\nLieutenant\n\n\n\n\n\nIt is reccommend that the city look to hire additional staff in these positions to lower the amount of overtime hours that are accrued in these positions.\n If the city were to hire additional fulltime employees to cover all the overtime hours that were accumulated over the past 10 years, they would need to hire 202,462 more full time employees. Below outlines the total savings per agency, as well as the amount of additional full time employees needed to cover the overtime hours.\n\n\n\n\n\n\n This would have saved the city $7,452,570,913."
  },
  {
    "objectID": "mp01.html#policy-iii",
    "href": "mp01.html#policy-iii",
    "title": "Mini Project 1: NYC Taxpayer Spending Analysis",
    "section": "Policy III:",
    "text": "Policy III:"
  },
  {
    "objectID": "mp01.html#policy-iii-tierd-bonus-system-to-improve-retention-rate",
    "href": "mp01.html#policy-iii-tierd-bonus-system-to-improve-retention-rate",
    "title": "Mini Project 1: NYC Taxpayer Spending Analysis",
    "section": "Policy III: Tierd Bonus System to Improve Retention Rate",
    "text": "Policy III: Tierd Bonus System to Improve Retention Rate\nOn average in New York City public sector, it can cost anywhere from 20%-30% of a positions salary to hire an individual for that position. In order to reduce costs and make sure the payroll system is as efficient as possible, the City must pay closer attention to retention rates among agencies to avoid these additional costs of hiring new indviduals. \nBelow are the top 10 positions with the most turover, as well as the total savings each agency could have if they did not have to spend money to hire new people to replace those who have left.\nTop Ten Positions with most turnover:\n\n\n\n\n\n\n Agency Savings"
  },
  {
    "objectID": "mp01.html#policy-iii-improve-retention-rates-to-increase-savings",
    "href": "mp01.html#policy-iii-improve-retention-rates-to-increase-savings",
    "title": "Mini Project 1: NYC Taxpayer Spending Analysis",
    "section": "Policy III: Improve Retention Rates to Increase Savings",
    "text": "Policy III: Improve Retention Rates to Increase Savings\nOn average in the New York City public sector, it can cost anywhere from 20%-30% of a positions salary to hire an individual for that position. In order to reduce costs and make sure the payroll system is as efficient as possible, the City must pay closer attention to retention rates amongst agencies to avoid additional costs of hiring new indviduals to replace those who have quit. \nBelow are the top 10 positions with the most turnover, as well as the total savings each agency could have if they did not need to spend money to hire as many new employees.\nTop Ten Positions with most turnover:\n\n\n\n\n\n\n Agency Savings:\n\n\n\n\n\n\n A way that the city could promote employee retention is to implement a tiered bonus system. Using the savings from not having to hire new employees to replacce those who have quit, each agency could reward employees for the amount of time they stay at their job/within the job network covered by the New York City payroll system. For example, employees could recieve bonuses for staying 3.5 years, 5 years, 10 years, 15 years, and so on. This would not only help employee retention but also keep the money the City has within their budgets and not spent on new hire materials."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "2025 Green Transit Honors",
    "section": "",
    "text": "The Transit ALlance for Investigation of Variance is pround to announce this years winners of the Green Transit Honors.\nThese awards aim to recognize some of the most sustainable and environmentally friendly public transportation agencies in the U.S. These awards strive to honor those systems that have demonstrated a commitment to reducing carbon emmions and enhance energy effiency. \nIn an efforts to recognize agencies appropriately, for the first time ever, the awards have been broken out into three categories - Small Agencies, Medium Agencies, and Large Agencies. These new categories even the playing field in the selection process, allowing for a more fair and equal comparison during the consideration process. Requirements for each category can be found in the appendix, as well as further explanation of how data was collected and processed in order to select the winners."
  },
  {
    "objectID": "mp02.html#transit-alliance-for-investigation-of-variance-gta-iv-announces-the-winners-of-the-2025-green-transit-honors",
    "href": "mp02.html#transit-alliance-for-investigation-of-variance-gta-iv-announces-the-winners-of-the-2025-green-transit-honors",
    "title": "2025 Green Transit Honors",
    "section": "Transit Alliance for Investigation of Variance (GTA IV) Announces the winners of the 2025 Green Transit Honors",
    "text": "Transit Alliance for Investigation of Variance (GTA IV) Announces the winners of the 2025 Green Transit Honors\n\nThe Transit ALlance for Investigation of Variance is pround to announce this years winners of the Green Transit Honors.\nThese awards aim to recognize some of the most sustainable and environmentally friendly public transportation agencies in the U.S. These awards strive to honor those systems that have demonstrated a commitment to reducing carbon emmions and enhance energy effiency. \nIn an efforts to recognize agencies appropriately, for the first time ever, the awards have been broken out into three categories - Small Agencies, Medium Agencies, and Large Agencies. These new categories even the playing field in the selection process, allowing for a more fair and equal comparison during the consideration process. Requirements for each category can be found in the appendix, as well as further explanation of how data was collected and processed in order to select the winners."
  },
  {
    "objectID": "mp02.html#appendix",
    "href": "mp02.html#appendix",
    "title": "2025 Green Transit Honors",
    "section": "Appendix",
    "text": "Appendix\nAs part of the selection process for the 2025 Green Transit Honors, Transit Alliance for Investigation of Variance used data from the State Electricity Profiles, 2023 Annual Database Energy Consumption report, and the 2023 Service by Agency report.\n\nAnalysis began by downloading the relevant data and performing some inital exploratory analysis.\nElectricity at the state level was examined and the following are the findings from that analysis:\n- State with most expensive retail electricity: Hawaii\n- State with “dirtiest” electric mix: West Virginia\n- Average pounds of CO2 emitted per MWh of electricity produced in the U.S.: 1.016615e-05\n- Rarest primary energy source and cost/location: Petroleum used in Hawaii, cost $1,444\n- How many times cleaner is New York’s energy mix than that of Texas: 37.12%\nThe next section of analysis explored the NTD Service Data.\n- Transit service with the most UPT annually: MTA - Average trip length of a trip on MTA NYC: 3.644089 miles\n- Transit service in NYC that has the longest average trip length: MTA Long Island Railroad\n- State with the fewest total miles travelled by public transit: New Hampshire\n It is also important to note that not all states are captured in this data. There are 19 states omitted from the NTD Service data.\nThis was the first year that awards were broken into categories by size. The size of the agency was determined by looking at the total UPT of that agency  The ranges for the categories are:\nSmall: 7,343 - 152,982 UPT\nMedium: 1,690,548 - 14,469,285 UPT\nLarge: 14,715,363 - 21,056,024,352 UPT\nSources: https://www.eia.gov/environment/emissions/co2_vol_mass.php https://www.nhtsa.gov/press-releases/usdot-announces-new-vehicle-fuel-economy-standards-model-year-2024-2026\nhttps://www.azocleantech.com"
  },
  {
    "objectID": "mp02.html#footnotes",
    "href": "mp02.html#footnotes",
    "title": "2025 Green Transit Honors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA-Z↩︎"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "The Ultimate Spotify Playlist",
    "section": "",
    "text": "A music playlist can make or break the mood at a party or even just a person’s individual mood. The right playlist can help set the mood or vibe for any occasion. There is no better feeling than when you have a playlist where bop after bop comes on and absolutely no skips. However, one wrong song can completely switch up the vibe — and not for the better.\nUsing the Spotify Song Analytics dataset, as well as the Spotify Million Playlist Dataset, I have set out to curate the ultimate playlist for good vibes this spring. This playlist will make you want to roll down the windows as you drive or just have a good time. It’ll help set the ultimate good vibes for the upcoming warmer seasons."
  },
  {
    "objectID": "mp03.html#getting-the-data-inital-exploratory",
    "href": "mp03.html#getting-the-data-inital-exploratory",
    "title": "The Ultimate Spotify Playlist",
    "section": "Getting the Data & Inital Exploratory",
    "text": "Getting the Data & Inital Exploratory\nIn order to start to curate a playlist for good vibes this spring, I started by downloading the Spotify song analytics data set from user gabminamendez on github and processing it, which included reformatting the data so that each artist had its own line in the data set. An example of the first few rows of that data set can be seen below.\n\n\n\n\n\n\n Next, I downloaded and processed the Spotify Million Playlist dataset and rectangled the data into an more standard format that was easier to read."
  },
  {
    "objectID": "mp03.html#getting-the-data",
    "href": "mp03.html#getting-the-data",
    "title": "The Ultimate Spotify Playlist",
    "section": "Getting the Data",
    "text": "Getting the Data\nIn order to begin curating a playlist for good vibes this spring, I started by downloading the Spotify Song Analytics dataset from user gabminamendez on GitHub and processing it. This included reformatting the data so that each artist had their own line in the dataset. An example of the first few rows of that dataset can be seen below.\n\n\n\n\n\n\n Next, I downloaded and processed the Spotify Million Playlist dataset and reshaped the data into a more standard format that was easier to read.\nThis gave me 13,000 playlists to work with."
  },
  {
    "objectID": "mp03.html#inital-exploration",
    "href": "mp03.html#inital-exploration",
    "title": "The Ultimate Spotify Playlist",
    "section": "Inital Exploration",
    "text": "Inital Exploration\nNow to dive into the data.\nOut of the 13,000 playlists from the Spotify Millions data:\n\nThere are 206,451 distinct tracks\nThere are 42,327 distinct artists\nThe 5 most popular tracks are:\n\nHUMBLE. - Kendrick Lamar with 589 appearances\nOnce Dance - Drake with 562 appearances\nCloser - The Chainsmokers with 543 appearances\nBroccoli (feat. Lil Yachty) - DRAM with 525 appearances\nCongratulations - Post Malone with 511 appearances\n\nThe most popular song in a playlist that doesn’t have corresponding song charaterisitcs data is One Dance by Drake\nThe song with the highest danceability is Funky Cold Medina by Tone-Loc\nThe playlist with the longest average track length is “Sleep”\nThe most popular playlist is TOP POP with 15,842 followers"
  },
  {
    "objectID": "mp03.html#what-makes-a-popular-song",
    "href": "mp03.html#what-makes-a-popular-song",
    "title": "The Ultimate Spotify Playlist",
    "section": "What makes a popular song?",
    "text": "What makes a popular song?\nThe next phase of exploartion involved looking at characteristics of popular songs.\nThe next phase of exploration involved looking at characteristics of popular songs.  I started by looking at the correlation between song popularity and playlist appearances. For the sake of this exploration, a song with a popularity score of 75 or higher is considered “popular.”\n The data showed there is little correlation between how popular a song is and the number of playlist appearances.  The song Champions by Kanye West had the most playlist appearances but only had a popularity rating of 68.\nAdditionally, Goosebumps by Travis Scott had the highest popularity score (92) but only appeared in 410 playlists.   Looking at the years when the most popular songs were released, the chart below shows that 2017 had the highest number of popular songs released in a single year.\n  Next, I looked at the danceability of the songs in this dataset.\n Danceability peaked in 2017.   As for the different decades represented:\n Songs from the 2010s were the most frequently included in playlists.   The frequency of musical keys is also important when it comes to popular songs.\n Songs in Key 1 and Key 7 were the most frequent in these playlists.   As for track length:\n The most popular tracks are between 3–5 minutes long\n and have an average popularity score of 64.4.   Knowing which artists have the most songs that are considered “popular” is important as well. The chart below shows the top 10 artists by average popularity rating."
  },
  {
    "objectID": "mp03.html#building-the-playlist",
    "href": "mp03.html#building-the-playlist",
    "title": "The Ultimate Spotify Playlist",
    "section": "Building the Playlist",
    "text": "Building the Playlist\nNow that I’ve explored the data and evaluated the different properties of popular songs in these playlists, it’s time to start crafting the playlist. I began by choosing two anchor songs: “Closer” by The Chainsmokers and “YOUTH” by Troye Sivan.\n\nWhy these songs?\nThey remind me of the first warm days of spring—when all you want to do is be outside, hanging out in the sun, listening to chill music.  Of rolling the windows down, driving to the beach to meet up with friends.  Of just having a good time.   I started the curation by looking at songs that appeared on the same playlists as these two.\nI also identified artists that frequently appeared on those playlists and made note of them to assist in filtering.  Next, I looked at the release years.   “Closer” and “YOUTH” were released in 2016 and 2015, respecitvely. I’ve always had a soft spot for songs from those years-so this became the first filter for the platlist.   After filtering down to songs from 2015 and 2016, I focused on tempo:  “Closer” has a tempo of 95 BPM, and “YOUTH” is 91.5 BPM. To maintain a consistent vibe, I filtered for songs with tempos between 90 and 110 BPM.   Then I examined danceability.  Both songs have danceability scores between 0.62 and 0.75, so I filtered down to songs with a danceability score above 0.6.\nThis process gave me a list of about 28 artists.\n\n\n\n\n\n\n To narrow it down further, I filtered by acousticness.  “Closer” has an acousticness score of 0.414, and “YOUTH” is 0.0625.  To match the energy and feel of those tracks, I filtered for songs with acousticness at or below 0.42.   Most of these tracks fall under pop, dance, or house genres.  As a final step, I excluded artists that didn’t match this vibe—such as Chris Brown or Zac Brown Band.\nThis filtering process resulted in my final playlist of 16 songs, titled…"
  },
  {
    "objectID": "mp03.html#good-vibes-2016-edition",
    "href": "mp03.html#good-vibes-2016-edition",
    "title": "The Ultimate Spotify Playlist",
    "section": "Good Vibes, 2016 Edition",
    "text": "Good Vibes, 2016 Edition\n   This curated collection captures the essence of mid-2010s pop and electronic music. Featuring tracks from artists like The Chainsmokers, Kygo, Troye Sivan, and Fifth Harmony, it offers a nostalgic journey through the upbeat and chill sounds that defined 2015 and 2016. A blend of synth-pop, tropical house, and mellow electronic beats, this playlist is perfect for relaxed listening sessions, evoking memories of summer evenings and carefree moments."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Exploring Political Shift",
    "section": "",
    "text": "When an election comes around, especially a presidential election, one of the key elements of its coverage is transforming the data into interesting visuals. This often takes the form of charts and graphs designed to make complex information and trends easier to understand. One of the most iconic graphics to emerge from recent election reporting is the New York Times “Shift” map, which demonstrates how voters’ preferences have shifted compared to previous elections. In this mini project, I will use county-level results from the 2020 and 2024 presidential elections to recreate this visual."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Exploring Political Shift",
    "section": "",
    "text": "When an election comes around, especially a presidential election, one of the key elements of its coverage is transforming the data into interesting visuals. This often takes the form of charts and graphs designed to make complex information and trends easier to understand. One of the most iconic graphics to emerge from recent election reporting is the New York Times “Shift” map, which demonstrates how voters’ preferences have shifted compared to previous elections. In this mini project, I will use county-level results from the 2020 and 2024 presidential elections to recreate this visual."
  },
  {
    "objectID": "mp04.html#gathering-the-data",
    "href": "mp04.html#gathering-the-data",
    "title": "Exploring Political Shift",
    "section": "Gathering the Data",
    "text": "Gathering the Data\nFor this project, as mentioned above, I am using county-level election results from the 2020 and 2024 presidential elections, along with a shapefile contatining U.S. county boundaries from the U.S. Census Bureau to help construct the visual map.  The first step in this process is to download the shapefile.\n\n\nShow the code\n#task 1 - US County Shapefile \nif (!require(\"utils\")) install.packages(\"utils\", dependencies = TRUE)\n\n# Set directory and file info\ndir_path &lt;- \"data/mp04\"\nfile_name &lt;- \"cb_2023_us_county_5m.zip\"\nfile_url &lt;- paste0(\"https://www2.census.gov/geo/tiger/GENZ2023/shp/\", file_name)\nfile_path &lt;- file.path(dir_path, file_name)\n\n# Create directory if it doesn't exist\nif (!dir.exists(dir_path)) {\n  dir.create(dir_path, recursive = TRUE)\n  cat(\"Created directory:\", dir_path, \"\\n\")\n}\n\n#Download file if it doesn't already exist\nif (!file.exists(file_path)) {\n  download.file(file_url, destfile = file_path, mode = \"wb\")\n  cat(\"Downloaded shapefile to:\", file_path, \"\\n\")\n} else {\n  cat(\"Shapefile already exists at:\", file_path, \"\\n\")\n}\n\n#Unzip the shapefile if not already unzipped\nunzipped_files &lt;- list.files(dir_path, pattern = \"cb_2023_us_county_5m.*\\\\.shp$\", full.names = TRUE)\nif (length(unzipped_files) == 0) {\n  unzip(file_path, exdir = dir_path)\n  cat(\"Unzipped shapefile into:\", dir_path, \"\\n\")\n} else {\n  cat(\"Shapefile already unzipped in:\", dir_path, \"\\n\")\n}\n\n\nThe next step is to begin gathering the election data.\nI started by developing a function called get_state_election_results, which downloads data from the from the 2024 presidential election on Wikipedia.\n\n\nShow the code\n#Task 2 - Acquire 2024 use Presidential Election Results \n#download libraries\nlibrary(httr2)\nlibrary(rvest)\nlibrary(readr)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(DT)\nlibrary(sf)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n#Create directory to store raw HTMLs\ndir.create(\"data/election_pages\", recursive = TRUE, showWarnings = FALSE)\n\n# Helper function: generate Wikipedia slug\nget_wikipedia_slug &lt;- function(state_name) {\n  slug &lt;- str_replace_all(state_name, \" \", \"_\")\n  paste0(\"2024_United_States_presidential_election_in_\", slug)\n}\n\n# Function to fetch and clean county-level election data for one state\nget_state_election_results &lt;- function(state_name) {\n  slug &lt;- get_wikipedia_slug(state_name)\n  page_url &lt;- paste0(\"https://en.wikipedia.org/wiki/\", slug)\n  file_path &lt;- file.path(\"data/election_pages\", paste0(slug, \".html\"))\n  \n  # Download and cache HTML\n  if (!file.exists(file_path)) {\n    resp &lt;- request(page_url) |&gt; req_perform()\n    writeBin(resp_body_raw(resp), file_path)\n    message(\"Downloaded and saved: \", state_name)\n  } else {\n    message(\"Using cached: \", state_name)\n  }\n  \n  # Read page and parse tables\n  page &lt;- read_html(file_path)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table(fill = TRUE)\n  \n  # Identify the county-level results table\n  target_table &lt;- NULL\n  for (tbl in tables) {\n    col_names &lt;- tolower(names(tbl))\n    if (any(str_detect(col_names, \"county|parish|borough|city and borough\"))) {\n      target_table &lt;- tbl\n      break\n    }\n  }\n  \n  if (is.null(target_table)) {\n    warning(\"No county-level results table found for: \", state_name)\n    return(NULL)\n  }\n  \n  # Clean column names and add state\n  names(target_table) &lt;- make.names(names(target_table), unique = TRUE)\n  \n  clean_table &lt;- target_table |&gt;\n    rename_with(~str_replace_all(tolower(.x), \"\\\\s+\", \"_\")) |&gt;\n    mutate(state = state_name)\n  \n  return(clean_table)\n}\n\n\nI then used this function to gather all county-level results from all 50 states and combined them into a single dataset.\n\n\nShow the code\n#get results for all 50 states \nstates &lt;- state.name  \n\n# Fetch and store results\nall_results &lt;- lapply(states, function(st) {\n  tryCatch(\n    get_state_election_results(st),\n    error = function(e) {\n      message(\"Failed to get data for \", st, \": \", e$message)\n      return(NULL)\n    }\n  )\n})\n\ncombined_results_2024 &lt;- bind_rows(all_results)\n\n\nThis gave the below results for 2024:\n\n\n\n\n\n\nThe next step was to repeat this process to obtain county-level data from the 2020 presidential election for all 50 states.\n\n\nShow the code\n#Task 3 Acquire 2020 US Presidential Election Results\n# Create directory to store raw HTMLs\ndir.create(\"data/election_pages_2020\", recursive = TRUE, showWarnings = FALSE)\n\n# Wikipedia slug generator for 2020 pages\nget_2020_slug &lt;- function(state_name) {\n  slug &lt;- str_replace_all(state_name, \" \", \"_\")\n  paste0(\"2020_United_States_presidential_election_in_\", slug)\n}\n# Function to fetch and clean county-level election data for one state (2020)\nget_state_election_results_2020 &lt;- function(state_name) {\n  slug &lt;- get_2020_slug(state_name)\n  page_url &lt;- paste0(\"https://en.wikipedia.org/wiki/\", slug)\n  file_path &lt;- file.path(\"data/election_pages_2020\", paste0(slug, \".html\"))\n  \n  # Download and cache HTML\n  if (!file.exists(file_path)) {\n    resp &lt;- request(page_url) |&gt; req_perform()\n    writeBin(resp_body_raw(resp), file_path)\n    message(\"Downloaded and saved: \", state_name)\n  } else {\n    message(\"Using cached: \", state_name)\n  }\n  \n  # Read page and parse tables\n  page &lt;- read_html(file_path)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table(fill = TRUE)\n  \n  # Identify the county-level results table\n  target_table &lt;- NULL\n  for (tbl in tables) {\n    col_names &lt;- tolower(names(tbl))\n    if (any(str_detect(col_names, \"county|parish|borough|city and borough\"))) {\n      target_table &lt;- tbl\n      break\n    }\n  }\n  \n  if (is.null(target_table)) {\n    warning(\"No county-level results table found for: \", state_name)\n    return(NULL)\n  }\n  \n  # Clean column names and add state\n  names(target_table) &lt;- make.names(names(target_table), unique = TRUE)\n  \n  clean_table &lt;- target_table |&gt;\n    rename_with(~str_replace_all(tolower(.x), \"\\\\s+\", \"_\")) |&gt;\n    mutate(state = state_name)\n  \n  return(clean_table)\n}\n#get all 50 states \nstates &lt;- state.name\n\n# Fetch and store results\nall_results_2020 &lt;- lapply(states, function(st) {\n  tryCatch(\n    get_state_election_results_2020(st),\n    error = function(e) {\n      message(\"Failed to get data for \", st, \": \", e$message)\n      return(NULL)\n    }\n  )\n})\n\n# Combine into one data frame\ncombined_results_2020 &lt;- bind_rows(all_results_2020)\n\n\nThis gave the below results for 2020:\n\n\n\n\n\n\nOnce all the necessary data was acquired, I combined the 2020 data, 2024 data, and the shapefile into a single dataset.\n\n\nShow the code\n#Join the Tables & shape file \nJoined_election_results &lt;- combined_results_2020 |&gt;\n  inner_join(combined_results_2024, by = c( \"county\", \"state\"))\n\n#checking columns \nnames(Joined_election_results)\n\n#shapefile \nshapefile_path &lt;- list.files(\"data/mp04\", pattern = \"\\\\.shp$\", full.names = TRUE)\n# Read the shapefile\ncounty_sf &lt;- st_read(shapefile_path[1])\n\n#combine joined data with shape file:\nfull_combined &lt;- county_sf |&gt;\n  left_join(Joined_election_results, by = c(\"NAME\" = \"county\", \"STATE_NAME\" = \"state\"))\n\n\nNow with all the information in one place, I could begin some exploratory analysis."
  },
  {
    "objectID": "mp04.html#creating-the-graphic",
    "href": "mp04.html#creating-the-graphic",
    "title": "Exploring Political Shift",
    "section": "Creating the Graphic",
    "text": "Creating the Graphic\nNow that the data has been combined and throughly explored, it was time to recreate the NYT shift graphic.  I started updating the joined dataset and the shapefile to include the shift in votes towards Donald Trump in 2024.\n\n\nShow the code\njoined_for_figure &lt;- inner_join(county_sf, trump_shift, by = c( \"NAME\" = \"county\"))\n#drop na values\njoined_for_figure &lt;- joined_for_figure |&gt;\n  drop_na(change)\n\n\nThen, I repositioned Alaska and Hawaii to better fit the graphic.\n\n\nShow the code\nmerged &lt;- st_transform(joined_for_figure, crs = 2163)\n\nalaska &lt;- merged[merged$STATEFP == \"02\", ]\nhawaii &lt;- merged[merged$STATEFP == \"15\", ]\nlower48 &lt;- merged[!(merged$STATEFP %in% c(\"02\", \"15\")), ]\n\n\nalaska &lt;- st_transform(alaska, crs = 2163)\nlower48 &lt;- st_transform(lower48, crs = 2163)\n\nif (is.na(st_crs(hawaii))) {\n  st_crs(hawaii) &lt;- 2163  # Assign the same CRS as the rest\n}\n\nmerged &lt;- bind_rows(lower48, alaska, hawaii) |&gt;\n  st_transform(crs = 4326)\n\n\nAfter that, I found the centroids for all the counties.\n\n\nShow the code\n#centroids\ncentroids &lt;- st_centroid(merged)\nmax_shift &lt;- max(abs(merged$change), na.rm = TRUE)\ncentroids &lt;- centroids %&gt;%\n  mutate(\n    coords = st_coordinates(geometry),\n    x = coords[, 1],\n    y = coords[, 2],\n    xend = x + (change / max_shift) * 0.5,\n    yend = y\n  )\n\n\nAnd finally, I put it all together to create the graphic."
  },
  {
    "objectID": "mp04.html#initial-analysis",
    "href": "mp04.html#initial-analysis",
    "title": "Exploring Political Shift",
    "section": "Initial Analysis",
    "text": "Initial Analysis\nBased on inital exploration, I was able to uncover the following:  Renville County in North Dakota cast the most votes for Donald Trump in 2024. There were 933 votes, which was 82.2% of the vote in that county.\n\n\nShow the code\n#Which county or counties cast the most votes for Trump (in absolute terms) in 2024?\nmost_trump_2024 &lt;- combined_results_2024 |&gt;\n  filter(donald.trumprepublican == max(donald.trumprepublican, na.rm = TRUE))|&gt;\n  select(county, donald.trumprepublican, state)\n#Renville in North Dakota (933, 82.2% of the vote)\n\n\nKalawao County in Hawaii cast the most votes for Joe Biden (as a fraction of total votes cast) in 2020.\n\n\nShow the code\n#Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\ncombined_results_2020_joe &lt;- combined_results_2020 |&gt;\n  select(county, joe.bidendemocratic, state, total.votes.cast)|&gt;\n  drop_na(total.votes.cast)\n\n#convert to numbers for math\ncombined_results_2020_joe$total.votes.cast[combined_results_2020_joe$total.votes.cast == \"NA\"] &lt;- NA\ncombined_results_2020_joe$joe.bidendemocratic[combined_results_2020_joe$joe.bidendemocratic == \"NA\"] &lt;- NA\ncombined_results_2020_joe$total.votes.cast &lt;- as.numeric(combined_results_2020_joe$total.votes.cast)\ncombined_results_2020_joe$joe.bidendemocratic &lt;- as.numeric(combined_results_2020_joe$joe.bidendemocratic)\n\n#find the fraction and sort\ncombined_results_2020_joe &lt;- combined_results_2020_joe |&gt;\n  mutate(\n    biden_vote_fraction = joe.bidendemocratic /total.votes.cast\n  )|&gt;\n  arrange(desc(biden_vote_fraction))\n\n\nCostilla County in Colorado had the largest shift towards Donald Trump in 2024.\n\n\nShow the code\n#Which county or counties had the largest shift towards Trump (in absolute terms) in 2024?\ntrump_shift &lt;- Joined_election_results|&gt;\n  select(county, donald.trumprepublican.x,donald.trumprepublican.y, state)|&gt;\n  drop_na(donald.trumprepublican.x)|&gt;\n  filter(donald.trumprepublican.x != \"NA\", donald.trumprepublican.y != \"NA\")\n  \n\n#check to see if numeric for math\nis.numeric(trump_shift$donald.trumprepublican.x)\nis.numeric(trump_shift$donald.trumprepublican.y)\n\n#convert to numeric \ntrump_shift$donald.trumprepublican.x[trump_shift$donald.trumprepublican.x == \"NA\"] &lt;- NA\ntrump_shift$donald.trumprepublican.y[trump_shift$donald.trumprepublican.y == \"NA\"] &lt;- NA\n\ntrump_shift$donald.trumprepublican.x &lt;- gsub(\",\", \"\", trump_shift$donald.trumprepublican.x)\ntrump_shift$donald.trumprepublican.x &lt;- as.numeric(trump_shift$donald.trumprepublican.x)\n\ntrump_shift$donald.trumprepublican.y &lt;- gsub(\",\", \"\", trump_shift$donald.trumprepublican.y)\ntrump_shift$donald.trumprepublican.y &lt;- as.numeric(trump_shift$donald.trumprepublican.y)\n\n#find the shift\ntrump_shift &lt;- trump_shift |&gt;\n  mutate(\n    change = donald.trumprepublican.y - donald.trumprepublican.x\n  )|&gt;\n  drop_na(county)\nbiggest_shift&lt;- trump_shift|&gt;\n  arrange(desc(change))\n\n\nTexas had the biggest swing toward Kamala Harris in 2024.\n\n\nShow the code\n#Which state had the largest shift towards Harris (or smallest shift towards Trump) in 2024?\nharris_swing &lt;- trump_shift|&gt;\n  group_by(state)|&gt;\n  summarise(harris_change = sum(change, na.rm = TRUE))|&gt;\n  filter(harris_change == min(harris_change, na.rm = TRUE))\n\n\nYukon-Koyuku County in Alaska is the biggest county by area in this dataset. It has a total area of 380,504,049,918 square miles.\n\n\nShow the code\n#What is the largest county, by area, in this data set\narea_find &lt;-full_combined\n#find the area of each countt\narea_find$areafull &lt;- st_area(area_find$geometry)\n\nlargest_county &lt;- area_find |&gt;\n  select(NAME, STATE_NAME, areafull)|&gt;\n  arrange(desc(areafull))\n\n\nHarmon County in Oklahoma had the highest voter density (voters per unit of area) in 2020.\n\n\nShow the code\ntotal_voters_2020 &lt;- area_find|&gt;\n  select(NAME, total.x , STATE_NAME, areafull)|&gt;\n  filter(total.x != \"NA\")\n\nis.numeric(total_voters_2020$total.x)\n\ntotal_voters_2020$total.x[total_voters_2020$total.x == \"NA\"] &lt;- NA\ntotal_voters_2020$total.x &lt;- as.numeric(total_voters_2020$total.x)\n\n#convert to KM\ntotal_voters_2020$area_km2 &lt;- total_voters_2020$areafull / 1e6\n  \ntotal_voters_2020 &lt;- total_voters_2020 |&gt;\n  mutate(\n    voter_density = total.x / area_km2\n  )|&gt;\n  arrange(desc(voter_density))\n\n\nLoving County in Texas had the biggest increase in voter turn out in 2024.\n\n\nShow the code\n#Which county had the largest increase in voter turnout in 2024?\ncleaned_joined_election &lt;- Joined_election_results|&gt;\n  select(county, total.x ,total.y, state)|&gt;\n  filter( total.x != \"NA\", total.y != \"NA\")\n\n#change to numeric\ncleaned_joined_election$total.x[cleaned_joined_election$total.x == \"NA\"] &lt;- NA\ncleaned_joined_election$total.x&lt;- as.numeric(cleaned_joined_election$total.x)\n\ncleaned_joined_election$total.y[cleaned_joined_election$total.y == \"NA\"] &lt;- NA\ncleaned_joined_election$total.y&lt;- as.numeric(cleaned_joined_election$total.y)\n\n#find increase in voter turnout \ncleaned_joined_election &lt;- cleaned_joined_election |&gt;\n  mutate(\n    voter_turn_out = total.y - total.x\n  )|&gt;\n  filter(county != 'NA')|&gt;\n  arrange(desc(voter_turn_out))"
  }
]